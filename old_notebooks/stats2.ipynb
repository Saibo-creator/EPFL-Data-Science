{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_project-hlefebvr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=final_project-hlefebvr>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('final_project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '200')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pyspark.sql.functions as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_df = spark.read.csv(\"/datasets/project/istdaten/*/*/*.csv\", header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rename some useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BETRIEBSTAG', 'FAHRT_BEZEICHNER', 'BETREIBER_ID', 'BETREIBER_ABK', 'BETREIBER_NAME', 'PRODUKT_ID', 'LINIEN_ID', 'LINIEN_TEXT', 'UMLAUF_ID', 'VERKEHRSMITTEL_TEXT', 'ZUSATZFAHRT_TF', 'FAELLT_AUS_TF', 'BPUIC', 'HALTESTELLEN_NAME', 'ANKUNFTSZEIT', 'AN_PROGNOSE', 'AN_PROGNOSE_STATUS', 'ABFAHRTSZEIT', 'AB_PROGNOSE', 'AB_PROGNOSE_STATUS', 'DURCHFAHRT_TF']\n"
     ]
    }
   ],
   "source": [
    "oldColumns = whole_df.schema.names\n",
    "print(oldColumns)\n",
    "newColumns = [\"date\", 'trip_id', \n",
    "              'BETREIBER_ID', 'BETREIBER_ABK',\n",
    "              'BETREIBER_NAME', \"transport_type\", \n",
    "             \"train_line\", \"train_service\", \n",
    "              'UMLAUF_ID', 'VERKEHRSMITTEL_TEXT',\n",
    "             \"additional_trip\", \"failed_trip\",\n",
    "             'BPUIC', \"station_name\", \"arrival_time\",\n",
    "             \"actual_arrival\", 'AN_PROGNOSE_STATUS',\n",
    "             \"departure_time\", \"actual_departure\",\n",
    "             'AB_PROGNOSE_STATUS', \"DURCHFAHRT_TF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- BETREIBER_ID: string (nullable = true)\n",
      " |-- BETREIBER_ABK: string (nullable = true)\n",
      " |-- BETREIBER_NAME: string (nullable = true)\n",
      " |-- transport_type: string (nullable = true)\n",
      " |-- train_line: string (nullable = true)\n",
      " |-- train_service: string (nullable = true)\n",
      " |-- UMLAUF_ID: string (nullable = true)\n",
      " |-- VERKEHRSMITTEL_TEXT: string (nullable = true)\n",
      " |-- additional_trip: string (nullable = true)\n",
      " |-- failed_trip: string (nullable = true)\n",
      " |-- BPUIC: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- actual_arrival: string (nullable = true)\n",
      " |-- AN_PROGNOSE_STATUS: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- actual_departure: string (nullable = true)\n",
      " |-- AB_PROGNOSE_STATUS: string (nullable = true)\n",
      " |-- DURCHFAHRT_TF: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whole_df = whole_df.toDF(*newColumns)\n",
    "whole_df.printSchema()\n",
    "# whole_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dropping useless columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_df = whole_df.drop('BETREIBER_ID','BETREIBER_ABK', 'BETREIBER_NAME', 'UMLAUF_ID', 'BPUIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radius_stations_df = spark.read.csv('final_project/zurich_hb_stops.csv', header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_df = whole_df.join(radius_stations_df, on=\"station_name\", how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIRD TABLE: COMPUTING THE DELTAS DELAYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First assumptions: we filter entries were transport does not stop, entries that are additional trips and failed trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "third_table = filtered_df.filter((filtered_df.DURCHFAHRT_TF == False) \\\n",
    "                                 & (filtered_df.additional_trip == False)\\\n",
    "                                 & (filtered_df.failed_trip == False)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_trips = third_table.groupBy('trip_id').count().filter(\"count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_trips = useful_trips.drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merging with our third table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "third_table = third_table.join(useful_trips, on=\"trip_id\", how='inner').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we just use only the GESCHAETZT entries, we will use only 4.8 % of our available data. Also, if we drop all the other entries there is the risk to lose information about some connection in our network. Right now we will just compute the difference of the columns. A further discussion with the other group members and TAs should be done.\n",
    "- Is critical how to handle the null values, do we put delay = 0 if null? Or do we drop those entries? (by dropping them, we stil have the same problem highlighted in the first point of this cell. Right now I put a zero for all the null values.\n",
    "- We also create a 'hour' column because it could be useful for the statitistics tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,unix_timestamp,hour,to_timestamp,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "third_table_final = third_table\\\n",
    ".withColumn('arrival_delay', when((third_table.actual_arrival.isNull()) \\\n",
    "                                  & (third_table.arrival_time.isNull()), None\n",
    "                                 )\n",
    "                                 .when((third_table.actual_arrival.isNull()) \\\n",
    "                                       & (third_table.arrival_time.isNotNull()), 0)\\\n",
    "                                 .otherwise(functions.round(unix_timestamp(\"actual_arrival\",'dd.MM.yyyy HH:mm') - \\\n",
    "                                            unix_timestamp(\"arrival_time\",'dd.MM.yyyy HH:mm')) / 60))\\\n",
    "\n",
    ".withColumn('departure_delay', when((third_table.actual_departure.isNull())\\\n",
    "                                    & (third_table.departure_time.isNull()), None\n",
    "                                 )\n",
    "                                 .when((third_table.actual_departure.isNull()) \\\n",
    "                                       & (third_table.departure_time.isNotNull()), 0)\\\n",
    "                                    .otherwise(functions.round(unix_timestamp(\"actual_departure\",'dd.MM.yyyy HH:mm')\\\n",
    "                                               - unix_timestamp(\"departure_time\",'dd.MM.yyyy HH:mm')) /60))\\\n",
    ".withColumn('hour',  when(third_table.arrival_time.isNull(), hour(to_timestamp(third_table.departure_time,\n",
    "                                                                              'dd.MM.yyyy HH:mm'))) \\\n",
    "                         .otherwise(hour(to_timestamp(third_table.arrival_time, 'dd.MM.yyyy HH:mm'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATISTICS TETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we will produce statistics tests in order to see if our bins delays can fit well a known distribution (we were thinking about a logNormal distr).\n",
    "- First we will produce the test on the whole dataset, that means on the third table.\n",
    "- then we will run a test on the delays on each station (group by station name)\n",
    "- after this we will do the same for each transport line (group by the line_id, right now the columns is erroneusly called 'train_line')\n",
    "- then, the same thing for each trip (group by trip_id)\n",
    "- finally, same tests for hour of the day (we need to choose how to split the day, right now the column 'hour' has 24 distinct values of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from scipy.stats import lognorm\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'station_name',\n",
       " 'date',\n",
       " 'transport_type',\n",
       " 'train_line',\n",
       " 'train_service',\n",
       " 'VERKEHRSMITTEL_TEXT',\n",
       " 'additional_trip',\n",
       " 'failed_trip',\n",
       " 'arrival_time',\n",
       " 'actual_arrival',\n",
       " 'AN_PROGNOSE_STATUS',\n",
       " 'departure_time',\n",
       " 'actual_departure',\n",
       " 'AB_PROGNOSE_STATUS',\n",
       " 'DURCHFAHRT_TF',\n",
       " 'id',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'elevation',\n",
       " 'dist_to_zurich_HB',\n",
       " 'arrival_delay',\n",
       " 'departure_delay',\n",
       " 'hour']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_table_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we consider that all delay distributions are log normals, no matter the groupby ? No function to guess the distrib, only tests to assess normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns a tupple with two arrays, ticks and counts (pass number of bins into parameters, 20 here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp=third_table_final.select('departure_delay').rdd.flatMap(lambda x: x).histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delay_per_station=third_table_final.groupBy('station_name','departure_delay').count().orderBy('station_name').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def plot_hist_delay(bins,arrival_or_departure):\n",
    "    #temp=third_table_final.select(arrival_or_departure).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "   # plt.bar(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count_dep_delay_panda=third_table_final.groupBy('departure_delay').count().orderBy('departure_delay').toPandas()\n",
    "#count_arr_delay_panda=third_table_final.groupBy('arrival_delay').count().orderBy('arrival_delay').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fit the lognormfit to all the delays per trip_id and line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trip_window = Window.partitionBy('trip_id')\n",
    "#train_line_window = Window.partitionBy('train_line')\n",
    "#trip_stop_window=Window.partitionBy('trip_id','station_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over variables  for arrival delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dep_avg_tripid=avg(\"departure_delay\").over(trip_window)\n",
    "#dep_var_tripid=variance(\"departure_delay\").over(trip_window)\n",
    "#dep_avg_trainline=avg(\"departure_delay\").over(train_line_window)\n",
    "#dep_var_trainline=variance(\"departure_delay\").over(train_line_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over variables for departure delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arr_avg_tripid=avg(\"arrival_delay\").over(trip_window)\n",
    "#arr_var_tripid=variance(\"arrival_delay\").over(trip_window)\n",
    "#arr_avg_trainline=avg(\"arrival_delay\").over(train_line_window)\n",
    "#arr_var_trainline=variance(\"arrival_delay\").over(train_line_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build train line and trip id dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tripid_avg_var = third_table_final.select('trip_id','departure_delay','arrival_delay',dep_avg_tripid.alias('average_dep_delay'),dep_var_tripid.alias('variance_dep_delay'),arr_avg_tripid.alias('average_arr_delay'),arr_var_tripid.alias('variance_arr_delay'))\n",
    "#trainline_avg_var = third_table_final.select('train_line','departure_delay','arrival_delay',dep_avg_trainline.alias('average_dep_delay'),dep_var_trainline.alias('variance_dep_delay'),arr_avg_trainline.alias('average_arr_delay'),arr_var_trainline.alias('variance_arr_delay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we modify the dataset and gather for each trip_id and each train_line the delays as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train_line = trainline_avg_var.groupBy('train_line','average_dep_delay','variance_dep_delay','average_arr_delay','variance_arr_delay').agg(functions.collect_list(\"departure_delay\").alias('list_dep_delay'),functions.collect_list(\"arrival_delay\").alias('list_arr_delay'))\n",
    "\n",
    "#df_trip_id=tripid_avg_var.groupBy('trip_id','average_dep_delay','variance_dep_delay','average_arr_delay','variance_arr_delay').agg(functions.collect_list(\"departure_delay\").alias('list_dep_delay'),functions.collect_list(\"arrival_delay\").alias('list_arr_delay'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "third_table_final_dep=third_table_final.where(col(\"departure_delay\").isNotNull())\n",
    "third_table_final_arr=third_table_final.where(col(\"arrival_delay\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_delay_dep = third_table_final_dep.groupBy('trip_id','station_name').agg(functions.collect_list(\"departure_delay\").alias('list_dep_delay'))\n",
    "df_delay_arr = third_table_final_arr.groupBy('trip_id','station_name').agg(functions.collect_list(\"arrival_delay\").alias('list_arr_delay'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_delay_dep_param=df_delay_dep.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['list_dep_delay'],[float(b) for b in lognorm.fit(x['list_dep_delay'])]])\n",
    "df_delay_dep_param=df_delay_arr.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['list_arr_delay'],[float(b) for b in lognorm.fit(x['list_arr_delay'])]])\n",
    "\n",
    "\n",
    "df_delay_dep_fit=df_delay_dep_param.map(lambda x:(x[0],x[1],x[2],x[3])).toDF(['trip_id','station_name','list_dep_delay','fit_param_dep'])\n",
    "df_delay_arr_fit=df_delay_dep_param.map(lambda x:(x[0],x[1],x[2],x[3])).toDF(['trip_id','station_name','list_arr_delay','fit_param_arr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------------+--------------------+\n",
      "|        trip_id|       station_name|      list_arr_delay|       fit_param_arr|\n",
      "+---------------+-------------------+--------------------+--------------------+\n",
      "|85:11:13752:001|     Birmensdorf ZH|[2.0, 1.0, -1.0, ...|[0.64744702021595...|\n",
      "|85:11:13752:001|Bonstetten-Wettswil|[3.0, 3.0, 0.0, 1...|[0.64971286678057...|\n",
      "+---------------+-------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delay_arr_fit.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the parameters in 3 columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_delay_dep_param_split=df_delay_dep_fit.select('trip_id','station_name',df_delay_dep_fit[\"fit_param_dep\"].getItem(0).alias(\"shape_dep\"),df_delay_dep_fit[\"fit_param_dep\"].getItem(1).alias(\"mean_dep\"),df_delay_dep_fit[\"fit_param_dep\"].getItem(2).alias(\"std_dep\")).cache()\n",
    "df_delay_arr_param_split=df_delay_arr_fit.select('trip_id','station_name',df_delay_arr_fit[\"fit_param_arr\"].getItem(0).alias(\"shape_arr\"),df_delay_arr_fit[\"fit_param_arr\"].getItem(1).alias(\"mean_arr\"),df_delay_arr_fit[\"fit_param_arr\"].getItem(2).alias(\"std_arr\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------------+-------------------+------------------+\n",
      "|        trip_id|       station_name|         shape_dep|           mean_dep|           std_dep|\n",
      "+---------------+-------------------+------------------+-------------------+------------------+\n",
      "|85:11:13752:001|     Birmensdorf ZH|0.6474470202159504|-1.6494151038426064|2.5714673710314537|\n",
      "|85:11:13752:001|Bonstetten-Wettswil|0.6497128667805727|-0.7439320429616096|   2.7395542606923|\n",
      "+---------------+-------------------+------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delay_dep_param_split.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df3bis=df_delay_dep_param_split.select([col(c).cast(\"string\") for c in df_delay_dep_param_split.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df3bis.write.csv('departure_fit_delays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_join=df_delay_dep_param_split.join(df_delay_arr_param_split,on=['trip_id','station_name'],how='inner').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'station_name',\n",
       " 'shape_dep',\n",
       " 'mean_dep',\n",
       " 'std_dep',\n",
       " 'shape_arr',\n",
       " 'mean_arr',\n",
       " 'std_arr']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_param= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in df_join:\n",
    "    if row['trip_id'] in dict_param:\n",
    "        continue\n",
    "    dict_param[row['trip_id']]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for id_trip in dict_param:\n",
    "    df=df_join.filter(df_join['trip_id']==id_trip)\n",
    "    for row in df:\n",
    "        dict_param[id_trip][row['station_name']]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for id_trip in dict_param:\n",
    "    for stop in dict_param[id_trip]:\n",
    "            df=df_join.filter((df_join['trip_id']==id_trip)&(df_join['station_name']=stop)\n",
    "            dict_param[id_trip][stop]['shape_dep']=df.select('shape_dep').collect()\n",
    "            dict_param[id_trip][stop]['mean_dep']=df.select('mean_dep').collect()\n",
    "            dict_param[id_trip][stop]['std_dep']=df.select('std_dep').collect()\n",
    "            dict_param[id_trip][stop]['shape_arr']=df.select('shape_arr').collect()\n",
    "            dict_param[id_trip][stop]['mean_arr']=df.select('mean_arr').collect()\n",
    "            dict_param[id_trip][stop]['std_arr']=df.select('std_arr').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_join.rdd.saveAsPickleFile('parameters_fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Map probabilities function : 3 solutions - use RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First solution UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proba_values(w,x,y,z):\n",
    "    return lognorm.pdf(np.linspace(min(w),max(w),100),x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf_proba= functions.udf(proba_values, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_delay_dep_param_split.withColumn(\"probabilities\",udf_proba(df_delay_dep_param_split.list_dep_delay,df_delay_dep_param_split.shape,df_delay_dep_param_split.mean,df_delay_dep_param_split.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second solution apply function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_delay_dep_param_split.withColumn(\"probabilities\",lognorm.pdf(np.linspace(min(df_delay_dep_param_split['list_dep_delay']),max(df_delay_dep_param_split['list_dep_delay']),100),df_delay_dep_param_split['shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_delay_dep_param_split.withColumn(\"probabilities\",lognorm.pdf(np.linspace(min(df_delay_dep_param_split['list_dep_delay']),max(df_delay_dep_param_split['list_dep_delay']),100),df_delay_dep_param_split['shape'],df_delay_dep_param_split['mean'],df_delay_dep_param_split['std']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third solution use rdd: Good solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rdd_dep=df_delay_dep_param_split.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['shape'],x['mean'],x['std'],[float(b) for b in lognorm.pdf(np.linspace(min(x['list_dep_delay']),max(x['list_dep_delay']),100),x['shape'],x['mean'],x['std'])]])\n",
    "df_rdd_arr=df_delay_arr_param_split.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['shape'],x['mean'],x['std'],[float(b) for b in lognorm.pdf(np.linspace(min(x['list_arr_delay']),max(x['list_arr_delay']),100),x['shape'],x['mean'],x['std'])]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final_dep=df_rdd_dep.map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5])).toDF(['trip_id','station_name','shape','mean','std','probabilities'])\n",
    "df_final_arr=df_rdd_arr.map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5])).toDF(['trip_id','station_name','shape','mean','std','probabilities'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3bis=df3.fillna(0).select([col(c).cast(\"string\") for c in df3.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3bis.write.csv('parameters_lognorm_fit4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3pandas=df3.fillna(0).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END : CSV Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example \n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# generate log-normal distributed set of samples\n",
    "samples   = np.random.lognormal( mean=1., sigma=.4, size=10000 )\n",
    "\n",
    "# make a fit to the samples and generate the resulting PDF\n",
    "shape, loc, scale = scipy.stats.lognorm.fit( samples, floc=0 )\n",
    "x_fit       = np.linspace( samples.min(), samples.max(), 100 )\n",
    "samples_fit = scipy.stats.lognorm.pdf( x_fit, shape, loc=loc, scale=scale )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_fit#output of lognom.pdf is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To compute proba such as : t+w<s, we need the cumulative distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf_fitted=stats.lognorm.cdf(x_fit,scatter,loc,mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find P[x<=Y]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.lognorm(mean, std).pdf(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find P[x>Y]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.lognorm(mean, std).sf(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
