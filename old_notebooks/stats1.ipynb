{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_project-sciuto</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=final_project-sciuto>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('final_project-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '6g')\n",
    "conf.set('spark.executor.instances', '6')\n",
    "conf.set('spark.port.maxRetries', '200')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pyspark.sql.functions as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = spark.read.csv(\"/datasets/project/istdaten/*/*/*.csv\", header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rename some useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BETRIEBSTAG', 'FAHRT_BEZEICHNER', 'BETREIBER_ID', 'BETREIBER_ABK', 'BETREIBER_NAME', 'PRODUKT_ID', 'LINIEN_ID', 'LINIEN_TEXT', 'UMLAUF_ID', 'VERKEHRSMITTEL_TEXT', 'ZUSATZFAHRT_TF', 'FAELLT_AUS_TF', 'BPUIC', 'HALTESTELLEN_NAME', 'ANKUNFTSZEIT', 'AN_PROGNOSE', 'AN_PROGNOSE_STATUS', 'ABFAHRTSZEIT', 'AB_PROGNOSE', 'AB_PROGNOSE_STATUS', 'DURCHFAHRT_TF']\n"
     ]
    }
   ],
   "source": [
    "oldColumns = whole_df.schema.names\n",
    "print(oldColumns)\n",
    "newColumns = [\"date\", 'trip_id', \n",
    "              'BETREIBER_ID', 'BETREIBER_ABK',\n",
    "              'BETREIBER_NAME', \"transport_type\", \n",
    "             \"train_line\", \"train_service\", \n",
    "              'UMLAUF_ID', 'VERKEHRSMITTEL_TEXT',\n",
    "             \"additional_trip\", \"failed_trip\",\n",
    "             'BPUIC', \"station_name\", \"arrival_time\",\n",
    "             \"actual_arrival\", 'AN_PROGNOSE_STATUS',\n",
    "             \"departure_time\", \"actual_departure\",\n",
    "             'AB_PROGNOSE_STATUS', \"DURCHFAHRT_TF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- BETREIBER_ID: string (nullable = true)\n",
      " |-- BETREIBER_ABK: string (nullable = true)\n",
      " |-- BETREIBER_NAME: string (nullable = true)\n",
      " |-- transport_type: string (nullable = true)\n",
      " |-- train_line: string (nullable = true)\n",
      " |-- train_service: string (nullable = true)\n",
      " |-- UMLAUF_ID: string (nullable = true)\n",
      " |-- VERKEHRSMITTEL_TEXT: string (nullable = true)\n",
      " |-- additional_trip: string (nullable = true)\n",
      " |-- failed_trip: string (nullable = true)\n",
      " |-- BPUIC: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- actual_arrival: string (nullable = true)\n",
      " |-- AN_PROGNOSE_STATUS: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- actual_departure: string (nullable = true)\n",
      " |-- AB_PROGNOSE_STATUS: string (nullable = true)\n",
      " |-- DURCHFAHRT_TF: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whole_df = whole_df.toDF(*newColumns)\n",
    "whole_df.printSchema()\n",
    "# whole_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dropping useless columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = whole_df.drop('BETREIBER_ID','BETREIBER_ABK', 'BETREIBER_NAME', 'UMLAUF_ID', 'BPUIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_stations_df = spark.read.csv('final_project/zurich_hb_stops.csv', header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = whole_df.join(radius_stations_df, on=\"station_name\", how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIRD TABLE: COMPUTING THE DELTAS DELAYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First assumptions: we filter entries were transport does not stop, entries that are additional trips and failed trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_table = filtered_df.filter((filtered_df.DURCHFAHRT_TF == False) \\\n",
    "                                 & (filtered_df.additional_trip == False)\\\n",
    "                                 & (filtered_df.failed_trip == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_trips = third_table.groupBy('trip_id').count().filter(\"count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_trips = useful_trips.drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_trips = useful_trips.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153572"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_trips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merging with our third table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_table = third_table.join(useful_trips, on=\"trip_id\", how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we just use only the GESCHAETZT entries, we will use only 4.8 % of our available data. Also, if we drop all the other entries there is the risk to lose information about some connection in our network. Right now we will just compute the difference of the columns. A further discussion with the other group members and TAs should be done.\n",
    "- Is critical how to handle the null values, do we put delay = 0 if null? Or do we drop those entries? (by dropping them, we stil have the same problem highlighted in the first point of this cell. Right now I put a zero for all the null values.\n",
    "- We also create a 'hour' column because it could be useful for the statitistics tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,unix_timestamp,hour,to_timestamp,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_table_final = third_table\\\n",
    ".withColumn('arrival_delay', when((third_table.actual_arrival.isNull()) \\\n",
    "                                  & (third_table.arrival_time.isNull()), None\n",
    "                                 )\n",
    "                                 .when((third_table.actual_arrival.isNull()) \\\n",
    "                                       & (third_table.arrival_time.isNotNull()), 0)\\\n",
    "                                 .otherwise(functions.round(unix_timestamp(\"actual_arrival\",'dd.MM.yyyy HH:mm') - \\\n",
    "                                            unix_timestamp(\"arrival_time\",'dd.MM.yyyy HH:mm')) / 60))\\\n",
    "\n",
    ".withColumn('departure_delay', when((third_table.actual_departure.isNull())\\\n",
    "                                    & (third_table.departure_time.isNull()), None\n",
    "                                 )\n",
    "                                 .when((third_table.actual_departure.isNull()) \\\n",
    "                                       & (third_table.departure_time.isNotNull()), 0)\\\n",
    "                                    .otherwise(functions.round(unix_timestamp(\"actual_departure\",'dd.MM.yyyy HH:mm')\\\n",
    "                                               - unix_timestamp(\"departure_time\",'dd.MM.yyyy HH:mm')) /60))\\\n",
    ".withColumn('hour',  when(third_table.arrival_time.isNull(), hour(to_timestamp(third_table.departure_time,\n",
    "                                                                              'dd.MM.yyyy HH:mm'))) \\\n",
    "                         .otherwise(hour(to_timestamp(third_table.arrival_time, 'dd.MM.yyyy HH:mm'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATISTICS TETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we will produce statistics tests in order to see if our bins delays can fit well a known distribution (we were thinking about a logNormal distr).\n",
    "- First we will produce the test on the whole dataset, that means on the third table.\n",
    "- then we will run a test on the delays on each station (group by station name)\n",
    "- after this we will do the same for each transport line (group by the line_id, right now the columns is erroneusly called 'train_line')\n",
    "- then, the same thing for each trip (group by trip_id)\n",
    "- finally, same tests for hour of the day (we need to choose how to split the day, right now the column 'hour' has 24 distinct values of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from scipy.stats import lognorm\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'station_name',\n",
       " 'date',\n",
       " 'transport_type',\n",
       " 'train_line',\n",
       " 'train_service',\n",
       " 'VERKEHRSMITTEL_TEXT',\n",
       " 'additional_trip',\n",
       " 'failed_trip',\n",
       " 'arrival_time',\n",
       " 'actual_arrival',\n",
       " 'AN_PROGNOSE_STATUS',\n",
       " 'departure_time',\n",
       " 'actual_departure',\n",
       " 'AB_PROGNOSE_STATUS',\n",
       " 'DURCHFAHRT_TF',\n",
       " 'id',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'elevation',\n",
       " 'dist_to_zurich_HB',\n",
       " 'arrival_delay',\n",
       " 'departure_delay',\n",
       " 'hour']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_table_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we consider that all delay distributions are log normals, no matter the groupby ? No function to guess the distrib, only tests to assess normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns a tupple with two arrays, ticks and counts (pass number of bins into parameters, 20 here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp=third_table_final.select('departure_delay').rdd.flatMap(lambda x: x).histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delay_per_station=third_table_final.groupBy('station_name','departure_delay').count().orderBy('station_name').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def plot_hist_delay(bins,arrival_or_departure):\n",
    "    #temp=third_table_final.select(arrival_or_departure).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "   # plt.bar(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count_dep_delay_panda=third_table_final.groupBy('departure_delay').count().orderBy('departure_delay').toPandas()\n",
    "#count_arr_delay_panda=third_table_final.groupBy('arrival_delay').count().orderBy('arrival_delay').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fit the lognormfit to all the delays per trip_id and line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trip_window = Window.partitionBy('trip_id')\n",
    "#train_line_window = Window.partitionBy('train_line')\n",
    "#trip_stop_window=Window.partitionBy('trip_id','station_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over variables  for arrival delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dep_avg_tripid=avg(\"departure_delay\").over(trip_window)\n",
    "#dep_var_tripid=variance(\"departure_delay\").over(trip_window)\n",
    "#dep_avg_trainline=avg(\"departure_delay\").over(train_line_window)\n",
    "#dep_var_trainline=variance(\"departure_delay\").over(train_line_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over variables for departure delays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arr_avg_tripid=avg(\"arrival_delay\").over(trip_window)\n",
    "#arr_var_tripid=variance(\"arrival_delay\").over(trip_window)\n",
    "#arr_avg_trainline=avg(\"arrival_delay\").over(train_line_window)\n",
    "#arr_var_trainline=variance(\"arrival_delay\").over(train_line_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build train line and trip id dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tripid_avg_var = third_table_final.select('trip_id','departure_delay','arrival_delay',dep_avg_tripid.alias('average_dep_delay'),dep_var_tripid.alias('variance_dep_delay'),arr_avg_tripid.alias('average_arr_delay'),arr_var_tripid.alias('variance_arr_delay'))\n",
    "#trainline_avg_var = third_table_final.select('train_line','departure_delay','arrival_delay',dep_avg_trainline.alias('average_dep_delay'),dep_var_trainline.alias('variance_dep_delay'),arr_avg_trainline.alias('average_arr_delay'),arr_var_trainline.alias('variance_arr_delay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we modify the dataset and gather for each trip_id and each train_line the delays as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train_line = trainline_avg_var.groupBy('train_line','average_dep_delay','variance_dep_delay','average_arr_delay','variance_arr_delay').agg(functions.collect_list(\"departure_delay\").alias('list_dep_delay'),functions.collect_list(\"arrival_delay\").alias('list_arr_delay'))\n",
    "\n",
    "#df_trip_id=tripid_avg_var.groupBy('trip_id','average_dep_delay','variance_dep_delay','average_arr_delay','variance_arr_delay').agg(functions.collect_list(\"departure_delay\").alias('list_dep_delay'),functions.collect_list(\"arrival_delay\").alias('list_arr_delay'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_table_final_dep=third_table_final.where(col(\"departure_delay\").isNotNull())\n",
    "third_table_final_arr=third_table_final.where(col(\"arrival_delay\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_dep = third_table_final_dep.groupBy('trip_id','station_name').agg(functions.collect_list(\"departure_delay\").alias('list_dep_delay'))\n",
    "df_delay_arr = third_table_final_arr.groupBy('trip_id','station_name').agg(functions.collect_list(\"arrival_delay\").alias('list_arr_delay'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2270456"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_delay_dep.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "log_norm = udf(lambda data: [float(b) for b in lognorm.fit(data)])\n",
    "\n",
    "# df = sqlContext.createDataFrame([{'name': 'Alice', 'age': 1}])\n",
    "# df.withColumn(\"maturity\", maturity_udf(df.age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_delay_dep_fit.withColumn('fit_param_dep', log_norm(df_delay_dep_fit.list_dep_delay)).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-73b660f3bf1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_delay_dep_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_delay_dep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trip_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'station_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list_dep_delay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlognorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list_dep_delay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_delay_dep_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'limit'"
     ]
    }
   ],
   "source": [
    "df_delay_dep_param=df_delay_dep.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['list_dep_delay'],[float(b) for b in lognorm.fit(x['list_dep_delay'])]])\n",
    "df_delay_dep_param.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_dep_param=df_delay_dep.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['list_dep_delay'],[float(b) for b in lognorm.fit(x['list_dep_delay'])]])\n",
    "df_delay_dep_param=df_delay_arr.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['list_arr_delay'],[float(b) for b in lognorm.fit(x['list_arr_delay'])]])\n",
    "\n",
    "\n",
    "df_delay_dep_fit=df_delay_dep_param.map(lambda x:(x[0],x[1],x[2],x[3])).toDF(['trip_id','station_name','list_dep_delay','fit_param_dep'])\n",
    "df_delay_arr_fit=df_delay_dep_param.map(lambda x:(x[0],x[1],x[2],x[3])).toDF(['trip_id','station_name','list_arr_delay','fit_param_arr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_delay_dep_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|             trip_id|        station_name|      list_dep_delay|       fit_param_dep|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|     85:11:18224:002|     Zürich Wiedikon|[2.0, 0.0, 0.0, 1...|[7.26212273964261...|\n",
      "|     85:11:18229:001|     Zürich Wiedikon|[0.0, 0.0, 1.0, 0...|[0.61609873007367...|\n",
      "|     85:11:18254:001|    Zürich Flughafen|[2.0, -1.0, 1.0, ...|[17.2164692937266...|\n",
      "|     85:11:18255:002|     Zürich Oerlikon|[1.0, 0.0, 0.0, 0...|[0.29602804524553...|\n",
      "|     85:11:18323:002|         Glanzenberg|[2.0, 2.0, 2.0, 2...|[0.00682712951607...|\n",
      "|     85:11:18356:001|  Zürich Stadelhofen|[0.0, 3.0, 0.0, 0...|[0.31082114225854...|\n",
      "|     85:11:18374:001|   Zürich Hardbrücke|[0.0, 1.0, 0.0, 0...|[3.19656665315266...|\n",
      "|     85:11:18382:001|   Zürich Hardbrücke|[0.0, 0.0, 0.0, 0...|[10.8542994819191...|\n",
      "|     85:11:18519:001|   Urdorf Weihermatt|[1.0, 0.0, 0.0, 1...|[0.37644850835146...|\n",
      "|     85:11:18531:001| Bonstetten-Wettswil|[0.0, 0.0, 0.0, 0...|[0.37835145889309...|\n",
      "|     85:11:18549:001|         Wallisellen|               [2.0]|[1.33432236948151...|\n",
      "|     85:11:18594:001|   Urdorf Weihermatt|[0.0, 0.0, 0.0, 0...|[9.10824437304614...|\n",
      "|     85:11:18636:001|     Zürich Oerlikon|[0.0, 1.0, 2.0, -...|[0.70246168235459...|\n",
      "|     85:11:18643:001|   Küsnacht Goldbach|[0.0, 0.0, 0.0, 1...|[2.50504295870057...|\n",
      "|     85:11:18651:001|     Zürich Oerlikon|[-1.0, 0.0, -1.0,...|[7.37747060866428...|\n",
      "|     85:11:18653:001|  Zürich Stadelhofen|[2.0, 3.0, 0.0, 0...|[1.69592342069971...|\n",
      "|     85:11:18658:001|  Zürich Stadelhofen|[0.0, -1.0, -1.0,...|[4.84545775184171...|\n",
      "|     85:11:18663:001|           Zürich HB|[-1.0, 6.0, -1.0,...|[11.3994132421487...|\n",
      "|     85:11:18664:001|     Regensdorf-Watt|[1.0, 0.0, 0.0, 1...|[0.50395083478742...|\n",
      "|     85:11:18691:001|      Zürich Seebach|[1.0, 7.0, 2.0, 1...|[8.58287654259555...|\n",
      "|     85:11:18738:001|   Zürich Hardbrücke|[0.0, 0.0, 0.0, 0...|[2.25183138897464...|\n",
      "|     85:11:18744:001|             Opfikon|[1.0, 4.0, 0.0, 2...|[6.96668771900593...|\n",
      "|     85:11:18769:001|   Zürich Hardbrücke|[3.0, 8.0, 2.0, 3...|[0.39657512164089...|\n",
      "|     85:11:18823:001|  Zürich Wollishofen|[0.0, 1.0, 0.0, 0...|[0.32039096431975...|\n",
      "|     85:11:18824:001|         Wallisellen|[0.0, 5.0, 1.0, 0...|[0.48307755403250...|\n",
      "|     85:11:18826:002|           Zürich HB|[-2.0, 9.0, 3.0, ...|[0.83837377857100...|\n",
      "|     85:11:18830:001|           Dietlikon|[0.0, 0.0, 1.0, 0...|[9.99944787209415...|\n",
      "|     85:11:18848:001|  Zürich Wollishofen|[0.0, 0.0, 0.0, 0...|[0.41034025084042...|\n",
      "|     85:11:18869:002|             Thalwil|[0.0, 1.0, 0.0, 0...|[7.54784301877389...|\n",
      "|     85:11:18880:001|           Zürich HB|[-2.0, -1.0, -1.0...|[8.69234293868667...|\n",
      "|     85:11:18920:002|   Zürich Hardbrücke|[0.0, 0.0, 0.0, 0...|[2.86044765844249...|\n",
      "|     85:11:18940:001|     Zürich Oerlikon|[0.0, 0.0, 0.0, 0...|[0.44517592592108...|\n",
      "|     85:11:19227:001|   Zürich Altstetten|[0.0, 3.0, 2.0, 0...|[0.41883787721252...|\n",
      "|     85:11:19228:001|   Zürich Altstetten|[1.0, 0.0, 0.0, -...|[0.56636357263262...|\n",
      "|     85:11:19258:001|   Zürich Altstetten|[0.0, 0.0, 0.0, 0...|[11.9608408915474...|\n",
      "|     85:11:19272:001|           Schlieren|[0.0, 1.0, 0.0, 0...|[3.61720109869287...|\n",
      "|     85:11:19281:001|           Zürich HB|[0.0, 0.0, -1.0, ...|[3.78728028615292...|\n",
      "|     85:11:19431:001|         Wallisellen|[3.0, 1.0, 1.0, 1...|[0.50016792786791...|\n",
      "|     85:11:19433:001|           Zürich HB|[4.0, 0.0, 1.0, 0...|[0.75150177099969...|\n",
      "|     85:11:19469:001|           Zürich HB|[1.0, 2.0, 1.0, 0...|[0.38968635808331...|\n",
      "|     85:11:19470:001|   Urdorf Weihermatt|[2.0, 2.0, 1.0, 1...|[0.52042938207825...|\n",
      "|     85:11:19527:001|           Stettbach|     [3.0, 2.0, 4.0]|[10.1413261408725...|\n",
      "|     85:11:19568:001|   Zürich Hardbrücke|[0.0, 3.0, 1.0, 0...|[11.5397077120662...|\n",
      "|     85:11:19577:001|           Zürich HB|[1.0, 1.0, 2.0, -...|[0.41475367231500...|\n",
      "|     85:11:19677:001|  Zürich Stadelhofen|[3.0, 0.0, 1.0, 0...|[2.99661326335109...|\n",
      "|     85:11:19680:001|         Küsnacht ZH|[0.0, 0.0, 1.0, 0...|[3.46063716065299...|\n",
      "|     85:11:19683:002|        Erlenbach ZH|[0.0, 0.0, 0.0, 4...|[2.97409246725594...|\n",
      "|     85:11:19931:001|     Zürich Oerlikon|[0.0, 2.0, 0.0, 0...|[0.39468716404625...|\n",
      "|     85:11:19965:001|     Zürich Oerlikon|[0.0, 0.0, -1.0, ...|[0.34839819084545...|\n",
      "|     85:11:19980:001|           Dietlikon|[0.0, 0.0, 0.0, 0...|[3.01103173762669...|\n",
      "|     85:11:20437:002|          Rüschlikon|[0.0, 1.0, 0.0, 1...|[14.8900359495451...|\n",
      "|     85:11:20438:001|           Zürich HB|[-2.0, -1.0, 0.0,...|[0.53011773002973...|\n",
      "|     85:11:20448:001|          Rüschlikon|[1.0, 1.0, 0.0, 1...|[0.26783166260568...|\n",
      "|     85:11:20451:001|         Bassersdorf|[0.0, 1.0, 2.0, 1...|[0.54041111742650...|\n",
      "|     85:11:20464:002|          Rüschlikon|[2.0, 2.0, 4.0, 1...|[5.43571488513174...|\n",
      "|     85:11:20466:002|          Rüschlikon|[2.0, 1.0, 1.0, 0...|[0.50755108350458...|\n",
      "|     85:11:20472:001|    Zürich Flughafen|[0.0, 0.0, 0.0, 4...|[10.9834111321237...|\n",
      "|     85:11:20493:001|           Zürich HB|[1.0, -1.0, 0.0, ...|[0.19147086286056...|\n",
      "|      85:11:2061:001|   Zürich Altstetten|[-1.0, 0.0, 6.0, ...|[0.58153631639033...|\n",
      "|      85:11:2688:003|           Zürich HB|[-1.0, -1.0, 1.0,...|[10.2135982526706...|\n",
      "|       85:11:414:002|           Zürich HB|    [-2.0, 1.0, 0.0]|[6.46665244685722...|\n",
      "|       85:11:710:001|    Zürich Flughafen|[0.0, 0.0, 0.0, 0...|[0.50096981278915...|\n",
      "|85:3849:106455-03...|Zürich, Kreuzstrasse|[0.0, 1.0, 0.0, 1...|[6.68875980624481...|\n",
      "|85:3849:106456-03...|Zürich, Grimselst...|[-1.0, 0.0, 0.0, ...|[0.18189334959536...|\n",
      "|85:3849:106470-03...|Zürich, Fröhlichs...|[0.0, 1.0, 1.0, 1...|[13.9987334965056...|\n",
      "|85:3849:106472-03...| Zürich, Stauffacher|[1.0, 1.0, 2.0, 1...|[0.63703362788387...|\n",
      "|85:3849:106487-03...|Zürich, Grimselst...|[-1.0, 0.0, 1.0, ...|[15.6639158396207...|\n",
      "|85:3849:106489-03...| Zürich, Stauffacher|[0.0, 0.0, 0.0, 2...|[6.27542260746002...|\n",
      "|85:3849:106492-03...|    Zürich, Bellevue|[1.0, 2.0, 1.0, 2...|[0.33548707439873...|\n",
      "|85:3849:106492-03...|Zürich, Börsenstr...|[0.0, 2.0, 1.0, 2...|[0.13780690273224...|\n",
      "|85:3849:106494-03...|   Zürich, Lochergut|[0.0, -1.0, 1.0, ...|[0.00870914156702...|\n",
      "|85:3849:106505-19...|  Zürich, Höschgasse|               [0.0]|[1.58858982214012...|\n",
      "|85:3849:106508-03...|Zürich, Grimselst...|[0.0, 0.0, 0.0, 0...|[2.19575821358237...|\n",
      "|85:3849:106508-19...|Zürich, Wildbachs...|               [2.0]|[1.33432236948151...|\n",
      "|85:3849:106510-03...|Zürich, Kreuzstrasse|[3.0, 0.0, 0.0, 0...|[0.20522446475537...|\n",
      "|85:3849:106518-03...| Zürich, Lindenplatz|[1.0, 0.0, 1.0, 1...|[0.20004516617800...|\n",
      "|85:3849:106529-03...| Zürich, Stauffacher|[2.0, 0.0, 0.0, 2...|[10.4281035804256...|\n",
      "|85:3849:106553-03...|  Zürich, Höschgasse|[1.0, 1.0, -1.0, ...|[0.48417429901640...|\n",
      "|85:3849:106555-03...|     Zürich, Kappeli|[0.0, 1.0, 0.0, 0...|[2.57157058765724...|\n",
      "|85:3849:106558-03...|Zürich, Bachmatts...|[2.0, 2.0, 1.0, 1...|[0.42108598588215...|\n",
      "|85:3849:106560-03...|   Zürich, Opernhaus|[1.0, 0.0, 0.0, 0...|[0.01057089826897...|\n",
      "|85:3849:106561-03...|    Zürich, Bellevue|[0.0, 1.0, 2.0, 0...|[0.11473658857467...|\n",
      "|85:3849:106567-03...|     Zürich, Kappeli|[1.0, 0.0, 0.0, 1...|[0.00649651749634...|\n",
      "|85:3849:106588-03...|  Zürich, Letzigrund|[0.0, 0.0, 0.0, 1...|[0.06317888700023...|\n",
      "|85:3849:106603-28...|Zürich, Albisried...|               [0.0]|[1.58858982214012...|\n",
      "|85:3849:106610-03...|Zürich,Kalkbreite...|[1.0, 5.0, 0.0, 0...|[0.39837593632646...|\n",
      "|85:3849:106611-03...|Zürich, Fröhlichs...|[1.0, 1.0, 3.0, 1...|[0.55110423249322...|\n",
      "|85:3849:106613-03...|Zürich, Albisried...|[0.0, 0.0, 0.0, 0...|[0.51325719799594...|\n",
      "|85:3849:106615-03...| Zürich, Bürkliplatz|[0.0, 0.0, 5.0, 0...|[0.59617564941319...|\n",
      "|85:3849:106672-03...| Zürich, Paradeplatz|[0.0, 0.0, -1.0, ...|[0.00573193363752...|\n",
      "|85:3849:106675-03...| Zürich, Stauffacher|[1.0, 0.0, 2.0, 1...|[0.15733124022405...|\n",
      "|85:3849:106689-03...|Zürich,Kalkbreite...|[1.0, 1.0, 1.0, 0...|[0.00155688140602...|\n",
      "|85:3849:106705-03...|Zürich, Feldeggst...|[0.0, 0.0, 1.0, 0...|[0.00561109700834...|\n",
      "|85:3849:106708-03...| Zürich, Paradeplatz|[1.0, 0.0, 3.0, 1...|[0.40693936365787...|\n",
      "|85:3849:106712-03...|  Zürich, Letzigrund|[1.0, 1.0, 0.0, 0...|[4.25866168872093...|\n",
      "|85:3849:106741-03...|Zürich, Bezirksge...|[1.0, 0.0, 3.0, 1...|[0.24065024882012...|\n",
      "|85:3849:106754-03...| Zürich, Stauffacher|[1.0, 1.0, -1.0, ...|[5.61326100365038...|\n",
      "|85:3849:106755-03...|  Zürich, Höschgasse|[0.0, 1.0, 0.0, 1...|[0.14236421471336...|\n",
      "|85:3849:106762-03...|Zürich, Bachmatts...|[1.0, 2.0, 1.0, 1...|[0.10544549669045...|\n",
      "|85:3849:106763-03...|Zürich, Zypressen...|[1.0, 0.0, 0.0, 0...|[12.0468285976493...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the parameters in 3 columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_dep_param_split=df_delay_dep_fit.select('trip_id','station_name',df_delay_dep_fit[\"fit_param_dep\"].getItem(0).alias(\"shape_dep\"),df_delay_dep_fit[\"fit_param_dep\"].getItem(1).alias(\"loc_dep\"),df_delay_dep_fit[\"fit_param_dep\"].getItem(2).alias(\"scale_dep\"))\n",
    "df_delay_arr_param_split=df_delay_arr_fit.select('trip_id','station_name',df_delay_arr_fit[\"fit_param_arr\"].getItem(0).alias(\"shape_arr\"),df_delay_arr_fit[\"fit_param_arr\"].getItem(1).alias(\"loc_arr\"),df_delay_arr_fit[\"fit_param_arr\"].getItem(2).alias(\"scale_arr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o285.showString.\n: org.apache.spark.SparkException: Job 8 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1704)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d402a07c4b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_delay_dep_param_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o285.showString.\n: org.apache.spark.SparkException: Job 8 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1704)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "df_delay_dep_param_split.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df3bis=df_delay_dep_param_split.select([col(c).cast(\"string\") for c in df_delay_dep_param_split.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df3bis.write.csv('departure_fit_delays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join=df_delay_dep_param_split.join(df_delay_arr_param_split,on=['trip_id','station_name'],how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'station_name',\n",
       " 'shape_dep',\n",
       " 'loc_dep',\n",
       " 'scale_dep',\n",
       " 'shape_arr',\n",
       " 'loc_arr',\n",
       " 'scale_arr']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trip_ids = df_join.select('trip_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o628.count.\n: org.apache.spark.SparkException: Job 17 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1446)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1439)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1439)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1439)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1701)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d976d45a6bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_trip_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o628.count.\n: org.apache.spark.SparkException: Job 17 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1446)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1439)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1439)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1439)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1701)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "unique_trip_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_param= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in df_join:\n",
    "    if row['trip_id'] in dict_param:\n",
    "        continue\n",
    "    dict_param[row['trip_id']]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for id_trip in dict_param:\n",
    "    df=df_join.filter(df_join['trip_id']==id_trip)\n",
    "    for row in df:\n",
    "        dict_param[id_trip][row['station_name']]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for id_trip in dict_param:\n",
    "    for stop in dict_param[id_trip]:\n",
    "            df=df_join.filter((df_join['trip_id']==id_trip)&(df_join['station_name']=stop)\n",
    "            dict_param[id_trip][stop]['shape_dep']=df.select('shape_dep').collect()\n",
    "            dict_param[id_trip][stop]['mean_dep']=df.select('mean_dep').collect()\n",
    "            dict_param[id_trip][stop]['std_dep']=df.select('std_dep').collect()\n",
    "            dict_param[id_trip][stop]['shape_arr']=df.select('shape_arr').collect()\n",
    "            dict_param[id_trip][stop]['mean_arr']=df.select('mean_arr').collect()\n",
    "            dict_param[id_trip][stop]['std_arr']=df.select('std_arr').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_join.rdd.saveAsPickleFile('parameters_fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Map probabilities function : 3 solutions - use RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First solution UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proba_values(w,x,y,z):\n",
    "    return lognorm.pdf(np.linspace(min(w),max(w),100),x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf_proba= functions.udf(proba_values, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_delay_dep_param_split.withColumn(\"probabilities\",udf_proba(df_delay_dep_param_split.list_dep_delay,df_delay_dep_param_split.shape,df_delay_dep_param_split.mean,df_delay_dep_param_split.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second solution apply function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_delay_dep_param_split.withColumn(\"probabilities\",lognorm.pdf(np.linspace(min(df_delay_dep_param_split['list_dep_delay']),max(df_delay_dep_param_split['list_dep_delay']),100),df_delay_dep_param_split['shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_delay_dep_param_split.withColumn(\"probabilities\",lognorm.pdf(np.linspace(min(df_delay_dep_param_split['list_dep_delay']),max(df_delay_dep_param_split['list_dep_delay']),100),df_delay_dep_param_split['shape'],df_delay_dep_param_split['mean'],df_delay_dep_param_split['std']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third solution use rdd: Good solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rdd_dep=df_delay_dep_param_split.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['shape'],x['mean'],x['std'],[float(b) for b in lognorm.pdf(np.linspace(min(x['list_dep_delay']),max(x['list_dep_delay']),100),x['shape'],x['mean'],x['std'])]])\n",
    "df_rdd_arr=df_delay_arr_param_split.rdd.map(lambda x:[x['trip_id'], x['station_name'],x['shape'],x['mean'],x['std'],[float(b) for b in lognorm.pdf(np.linspace(min(x['list_arr_delay']),max(x['list_arr_delay']),100),x['shape'],x['mean'],x['std'])]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final_dep=df_rdd_dep.map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5])).toDF(['trip_id','station_name','shape','mean','std','probabilities'])\n",
    "df_final_arr=df_rdd_arr.map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5])).toDF(['trip_id','station_name','shape','mean','std','probabilities'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3bis=df3.fillna(0).select([col(c).cast(\"string\") for c in df3.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3bis.write.csv('parameters_lognorm_fit4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3pandas=df3.fillna(0).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END : CSV Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example \n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# generate log-normal distributed set of samples\n",
    "samples   = np.random.lognormal( mean=1., sigma=.4, size=10000 )\n",
    "\n",
    "# make a fit to the samples and generate the resulting PDF\n",
    "shape, loc, scale = scipy.stats.lognorm.fit( samples, floc=0 )\n",
    "x_fit       = np.linspace( samples.min(), samples.max(), 100 )\n",
    "samples_fit = scipy.stats.lognorm.pdf( x_fit, shape, loc=loc, scale=scale )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_fit#output of lognom.pdf is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To compute proba such as : t+w<s, we need the cumulative distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf_fitted=stats.lognorm.cdf(x_fit,scatter,loc,mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find P[x<=Y]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.lognorm(mean, std).pdf(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find P[x>Y]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.lognorm(mean, std).sf(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
